#!/usr/bin/env python3

# Copyright (c) 2020 Danny van Dyk
#
# This file is part of the EOS project. EOS is free software;
# you can redistribute it and/or modify it under the terms of the GNU General
# Public License version 2, as published by the Free Software Foundation.
#
# EOS is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
# details.
#
# You should have received a copy of the GNU General Public License along with
# this program; if not, write to the Free Software Foundation, Inc., 59 Temple
# Place, Suite 330, Boston, MA  02111-1307  USA

import argparse
import eos
from eos import debug, info, warn, error
import logging
import numpy as _np
import os
import pypmc
import scipy
import sys
import yaml
from types import SimpleNamespace
import multiprocessing
import time

# return the value of the environment variable, or a default value if the variable is unset.
def get_from_env(envvar, default):
    if not envvar in os.environ:
        return default

    return os.environ[envvar]


def main():
    parser = argparse.ArgumentParser(description='Carry out a Bayesian analysis using EOS')
    subparsers = parser.add_subparsers(title = 'commands')

    ## begin of commands

    # list-priors
    parser_list_priors = subparsers.add_parser('list-priors',
        description = 'Lists the prior PDFs defined within the scope of this analysis.',
        help = 'list the known priors'
    )
    parser_list_priors.set_defaults(cmd = cmd_list_priors)

    # list-likelihoods
    parser_list_likelihoods = subparsers.add_parser('list-likelihoods',
        description = 'Lists the likelihoods defined within the scope of this analysis.',
        help = 'list the known likelihoods'
    )
    parser_list_likelihoods.add_argument('-d', '--display-details',
        help = 'display details for each likelihood',
        dest = 'display', action = 'store_true', default = False
    )
    parser_list_likelihoods.set_defaults(cmd = cmd_list_likelihoods)

    # list-posteriors
    parser_list_posteriors = subparsers.add_parser('list-posteriors',
        description = 'Lists the posteriors defined within the scope of this analysis.',
        help = 'list the known posteriors'
    )
    parser_list_posteriors.set_defaults(cmd = cmd_list_posteriors)

    # list-predictions
    parser_list_predictions = subparsers.add_parser('list-predictions',
        description = 'Lists the predictions defined within the scope of this analysis.',
        help = 'list the known predictions'
    )
    parser_list_predictions.set_defaults(cmd = cmd_list_predictions)

    # sample-mcmc
    parser_sample_mcmc = subparsers.add_parser('sample-mcmc',
        description = 'Samples from a posterior using Markov Chain Monte Carlo (MCMC) methods.',
        help = 'sample from a posterior using Markov chains'
    )
    parser_sample_mcmc.add_argument('posterior', metavar = 'POSTERIOR',
        help = 'posterior PDF from which to draw samples'
    )
    parser_sample_mcmc.add_argument('chain', metavar = 'CHAIN-IDX',
        help = 'index of the Markov chain (used to seed the RNG)'
    )
    parser_sample_mcmc.add_argument('-N', '--number-of-samples',
        help = 'number of samples to be drawn',
        dest = 'N', action = 'store', type = int, default = 1000
    )
    parser_sample_mcmc.add_argument('-S', '--stride',
        help = 'stride (i.e. number of samples to skip) to obtain final samples',
        dest = 'stride', action = 'store', type = int, default = 5
    )
    parser_sample_mcmc.add_argument('-p', '--number-of-preruns',
        help = 'number of times a prerun adapts to the posterior',
        dest = 'preruns', action = 'store', type = int, default = 3
    )
    parser_sample_mcmc.add_argument('-n', '--number-of-prerun-samples',
        help = 'number of samples to be drawn in each prerun (to be discarded)',
        dest = 'pre_N', action = 'store', type = int, default = 150
    )
    parser_sample_mcmc.add_argument('-b', '--base-directory',
        help = 'base directory where to store samples',
        dest = 'base_directory', action = 'store', default = get_from_env('EOS_BASE_DIRECTORY', './')
    )
    parser_sample_mcmc.set_defaults(cmd = cmd_sample_mcmc)

    # sample-pmc
    parser_sample_pmc = subparsers.add_parser('sample-pmc',
        description = 'Samples from a posterior using Population Monte Carlo (PMC) methods.',
        help = 'sample from a posterior using adaptive importance sampling'
    )
    parser_sample_pmc.add_argument('posterior', metavar = 'POSTERIOR',
        help = 'posterior PDF from which to draw samples'
    )
    parser_sample_pmc.add_argument('path', metavar = 'PATH',
        help = 'path to the output of \'find-cluster\''
    )
    parser_sample_pmc.add_argument('-n', '--number-of-adaptation-samples',
        help = 'number of samples to be drawn',
        dest = 'step_N', action = 'store', type = int, default = 500
    )
    parser_sample_pmc.add_argument('-s', '--number-of-adaptation-steps',
        help = 'number of samples to be drawn',
        dest = 'steps', action = 'store', type = int, default = 10
    )
    parser_sample_pmc.add_argument('-N', '--number-of-final-samples',
        help = 'number of samples to be drawn',
        dest = 'final_N', action = 'store', type = int, default = 5000
    )
    parser_sample_pmc.add_argument('-b', '--base-directory',
        help = 'base directory where to store samples',
        dest = 'base_directory', action = 'store', default = get_from_env('EOS_BASE_DIRECTORY', './')
    )
    parser_sample_pmc.set_defaults(cmd = cmd_sample_pmc)

    # plot-samples
    parser_plot_samples = subparsers.add_parser('plot-samples',
        description = 'Plots samples from a posterior.',
        help = 'plot samples'
    )
    parser_plot_samples.add_argument('path', metavar = 'PATH',
        help = 'path to the sampling data'
    )
    parser_plot_samples.add_argument('-b', '--bins',
        help = 'number of bins in the histogram',
        dest = 'bins', action = 'store', type = int, default = 50
    )
    parser_plot_samples.set_defaults(cmd = cmd_plot_samples)

    # find-mode
    parser_find_mode = subparsers.add_parser('find-mode',
        description = 'Finds the mode of a posterior.',
        help = 'find mode of posterior'
    )
    parser_find_mode.add_argument('posterior', metavar = 'POSTERIOR',
        help = 'posterior PDF from which to draw samples'
    )
    parser_find_mode.add_argument('-p', '--starting-points',
        help = 'number of points from which minization is started',
        dest = 'points', action = 'store', type = int, default = 10
    )
    parser_find_mode.add_argument('-i', '--init-from-file',
        help = 'name of the MCMC data file from which the minimization is started',
        dest = 'init_file', action = 'store', type = str, default = ''
    )
    parser_find_mode.set_defaults(cmd = cmd_find_mode)

    # find-clusters
    parser_find_clusters = subparsers.add_parser('find-clusters',
        description = 'Find clusters among the posterior samples by Gelman-Rubin R value.',
        help = 'find clusters'
    )
    parser_find_clusters.add_argument('output_path', metavar = 'OUTPUT_PATH',
        help = 'path to the storage location for the clusters',
        action = 'store', type = str
    )
    parser_find_clusters.add_argument('input_paths', metavar = 'INPUT_PATH',
        help = 'path to the sampling data',
        action = 'store', type = str, nargs='+',
    )
    parser_find_clusters.add_argument('-t', '--threshold',
        help = 'R value threshold',
        dest = 'threshold', action = 'store', type = float, default = 2.0
    )
    parser_find_clusters.add_argument('-c', '--clusters-per-group',
        help = 'number by which the final number of cluster shall be mulitplied',
        dest = 'K_g', action = 'store', type = int, default = 1
    )
    parser_find_clusters.set_defaults(cmd = cmd_find_clusters)

    # predict-observables
    parser_predict_observables = subparsers.add_parser('predict-observables',
        description = 'Predict a set of observables based on previously obtained PMC samples.',
        help = 'predict observables'
    )
    parser_predict_observables.add_argument('prediction', metavar = 'PREDICTION',
        help = 'name set of observables to predict',
        action = 'store', type = str
    )
    parser_predict_observables.add_argument('path', metavar = 'INPUT_PATH',
        help = 'path to the sampling data',
        action = 'store', type = str
    )
    parser_predict_observables.add_argument('-B', '--begin-index',
        help = 'index of the first sample to use for the predictions',
        dest = 'begin', action = 'store', type = int, default = 0
    )
    parser_predict_observables.add_argument('-E', '--end-index',
        help = 'index beyond the last sample to use for the predictions',
        dest = 'end', action = 'store', type = int, default = -1
    )
    parser_predict_observables.add_argument('-b', '--base-directory',
        help = 'base directory where to store samples',
        dest = 'base_directory', action = 'store', default = get_from_env('EOS_BASE_DIRECTORY', './')
    )
    parser_predict_observables.set_defaults(cmd = cmd_predict_observables)

    # egjvd2020 automisation
    parser_run = subparsers.add_parser('run',
        description = 'Runs a sequence of eos-analysis commands, which are issued in a given order.',
        help = 'run'
    )
    # parser_run.add_argument('template', metavar = 'TEMPLATE', #TODO: implement template run from yaml files 
    #     help = 'name set of observables to predict',
    #     action = 'store', type = str
    # )
    parser_run.add_argument('posterior', metavar = 'POSTERIOR',
        help = 'posterior PDF from which to draw samples'
    )
    parser_run.set_defaults(cmd = cmd_run)

    parser_batch_analysis = subparsers.add_parser('batch-analysis',
        description = 'Runs the anaylsis for a given batch of posteriors that is specified in the analysis file',
        help = 'batch-analyis'
    )
    parser_batch_analysis.set_defaults(cmd = cmd_batch_analysis)
    ## end of commands

    # add verbosity arg to all commands
    parsers = [
        parser,
        parser_find_clusters, parser_find_mode,
        parser_list_priors, parser_list_likelihoods, parser_list_posteriors, parser_list_predictions,
        parser_plot_samples,
        parser_sample_mcmc, parser_sample_pmc,
        parser_predict_observables,
        parser_run,
        parser_batch_analysis
    ]
    for p in parsers:
        p.add_argument('-v', '--verbose',
            help = 'increase verbosity',
            dest = 'verbose', action = 'count', default = 0
        )
        p.add_argument('-f', '--analysis-file',
            help = 'analysis file to be used',
            dest = 'analysis_file', action = 'store', default = '.analysis.yaml'
        )

    args = parser.parse_args()

    if args.verbose > 3:
        args.verbose = 3

    levels = {
        0: logging.ERROR,
        1: logging.WARNING,
        2: logging.INFO,
        3: logging.DEBUG
    }

    logging.basicConfig(level=levels[args.verbose])

    if not 'cmd' in args:
        parser.print_help()
    elif not callable(args.cmd):
        parser.print_help()
    else:
        args.cmd(args)


def make_analysis_file(args):
    analysis_file = eos.AnalysisFile(args.analysis_file)
    return analysis_file

def set_log_file(path, name='log'):
    os.makedirs(path, exist_ok=True)
    formatter = logging.Formatter('%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s')
    handler = logging.FileHandler(os.path.join(path, name), mode='w')
    handler.setFormatter(formatter)
    eos.logger.addHandler(handler)

class RNG:
    def __init__(self, seed):
        self._rng = _np.random.mtrand.RandomState(seed)

    def rand(self, *args):
        result = self._rng.rand(*args)
        print('rand: {}'.format(result))
        return result

    def normal(self, loc, scale=1, size=None):
        result = self._rng.normal(loc, scale, size)
        print('normal: {}'.format(result))
        return result

    def uniform(self, low, high, size=None):
        result = self._rng.uniform(low, high, size)
        print('uniform: {}'.format(result))
        return result


# Run sequence
def cmd_run(args, run_posterior = None, run_chain = 0): #TODO Generalise
    if run_posterior != None:
        args.posterior = run_posterior

    # define an object to be used as an argument for cmd_sample-mcmc
    mcmc_args = SimpleNamespace(
        #TODO make these parameters accesible for parsing arguments in terminal
        analysis_file = args.analysis_file,
        verbose = args.verbose,
        posterior = args.posterior,
        chain = run_chain, #todo
        N = 2500, #TODO change to higher value
        pre_N = 500, #TODO change to higher value
        preruns = 15,
        stride = 5, # default of parser_sample_mcmc
        base_directory = get_from_env('EOS_BASE_DIRECTORY', './')
    )
    # run markov chains
    cmd_sample_mcmc(mcmc_args)


    # define an object to be used as an argument for cmd_sample-mcmc
    find_mode_args = SimpleNamespace(
        analysis_file = args.analysis_file,
        verbose = args.verbose,
        posterior = args.posterior,
        points = 30, #TODO change to higher value
        init_file = os.path.join(args.posterior, 'mcmc-{:04}'.format(int(mcmc_args.chain)))
    )
    # run find-mode subroutine using previous markov chains
    data_dict = cmd_find_mode(find_mode_args, RETURN = True)

    #add additional data to data_dict
    data_dict["posterior"] = mcmc_args.posterior
    data_dict["chain"] = mcmc_args.chain
    data_dict["N"] = mcmc_args.N
    data_dict["pre_N"] = mcmc_args.pre_N
    data_dict["preruns"] = mcmc_args.preruns
    data_dict["points"] = find_mode_args.points
    saturation = 0
    for a_n in data_dict["bfp"]:
        saturation += a_n * a_n
    data_dict["bound-saturation"] = saturation


    #output log data
    text = ''
    for item in data_dict.items():
        text += str(item[0])+ ": " + str(item[1]) + '\n'
    txt_path = os.path.join(args.posterior, 'mcmc-{:04}'.format(int(mcmc_args.chain)), "fit_data.txt")
    txt_file = open(txt_path,"w")
    txt_file.write(text)
    txt_file.close()


    # create a .plot yaml file to be used in the plotting process
    yaml_template = open("plotting_template.plot", "r")
    plotting_string = yaml_template.read()
    yaml_template.close()

    bfp = data_dict["bfp"]
    for i in range(len(bfp)):
        if plotting_string.find("###TEMPLATE:INSERT_HERE###") != -1:
            plotting_string = plotting_string.replace("###TEMPLATE:INSERT_HERE###", str(bfp[i]), 1)
        else:
            error("'plotting_template.plot' contains insufficiently many insertion points")

    yaml_plot_file =  open(os.path.join(args.posterior, 'mcmc-{:04}'.format(int(mcmc_args.chain)), "figure.plot"),"w")
    yaml_plot_file.write(plotting_string)
    yaml_plot_file.close()

    # define an object to be used as an argument in plotting process
    # plot_args = SimpleNamespace(
    #     input = os.path.join(args.posterior, 'mcmc-{:04}'.format(int(mcmc_args.chain)), "figure.plot"), #Name of the plot input YAML file
    #     output = os.path.join(args.posterior, 'mcmc-{:04}'.format(int(mcmc_args.chain)), "plot.pdf") #Name of the plot output PDF file
    # )
    # if not os.path.isfile(plot_args.input): # ensure that the input file exists
    #     raise RuntimeError('\'%s\' is not a file' % plot_args.input)
    # instructions = None
    # with open(plot_args.input) as input_file:
    #     instructions = yaml.load(input_file)
    # # plot data
    # plotter = eos.plot.Plotter(instructions, plot_args.output)
    # plotter.plot()

    # create a .plot yaml file to be used in the plotting process for the phase
    yaml_template = open("plotting_template_phase.plot", "r")
    plotting_string = yaml_template.read()
    yaml_template.close()

    bfp = data_dict["bfp"]
    for i in range(len(bfp)):
        if plotting_string.find("###TEMPLATE:INSERT_HERE###") != -1:
            plotting_string = plotting_string.replace("###TEMPLATE:INSERT_HERE###", str(bfp[i]), 1)
        else:
            error("'plotting_template_phase.plot' contains insufficiently many insertion points")

    yaml_plot_file =  open(os.path.join(args.posterior, 'mcmc-{:04}'.format(int(mcmc_args.chain)), "figure2.plot"),"w")
    yaml_plot_file.write(plotting_string)
    yaml_plot_file.close()


    # define an object to be used as an argument in plotting process for the phase
    # plot_args = SimpleNamespace(
    #     input = os.path.join(args.posterior, 'mcmc-{:04}'.format(int(mcmc_args.chain)), "figure2.plot"), #Name of the plot input YAML file
    #     output = os.path.join(args.posterior, 'mcmc-{:04}'.format(int(mcmc_args.chain)), "plot2.pdf") #Name of the plot output PDF file
    # )
    # if not os.path.isfile(plot_args.input): # ensure that the input file exists
    #     raise RuntimeError('\'%s\' is not a file' % plot_args.input)
    # instructions = None
    # with open(plot_args.input) as input_file:
    #     instructions = yaml.load(input_file)
    # # plot data
    # plotter = eos.plot.Plotter(instructions, plot_args.output)
    # plotter.plot()

    return data_dict

def run_process(process_args, process_posterior, chain_index, PROCESS_COUNT, DataSets):
    PROCESS_COUNT.value += 1
    DataSets.append(cmd_run(process_args, run_posterior = process_posterior, run_chain = chain_index))
    PROCESS_COUNT.value += -1

def cmd_batch_analysis(args):
    #Execute run subroutine for given batch of posteriors
    analysis_file = make_analysis_file(args)
    #Starting Multiprocessing
    manager = multiprocessing.Manager()
    DataSets = manager.list()
    PROCESS_COUNT = manager.Value('i', 0)
    MAX_PROCESSES_COUNT = multiprocessing.cpu_count() -1
    ProcessList = []
    for posterior in analysis_file.batch:
        for i in range(64,128):
            ProcessList.append(multiprocessing.Process(target = run_process, args = (args, posterior,i, PROCESS_COUNT, DataSets)))
    counter = 0
    while counter < len(ProcessList):
        if PROCESS_COUNT.value < MAX_PROCESSES_COUNT:
            print("#######################################################################################PROCESS_COUNT:", PROCESS_COUNT.value)
            ProcessList[counter].start()
            counter += 1
            time.sleep(0.1)#needed to allow the Process-counter to update in time
            print("#######################################################################################PROCESS_COUNT:", PROCESS_COUNT.value)
        time.sleep(0.5)  #needed to keep the resources used by the loop itself at bay
    for i in range(len(ProcessList)):
        ProcessList[i].join()
    #End of Multiprocessing
    laTex_output = "posterior & chain index & $p$-value & $\chi^2$ & parameters & measurements & bound saturation" +'\n'
    for Set in DataSets:
        laTex_output += str(Set["posterior"]) + " & $" + str(Set["chain"]) + "$ & $" + str(round(Set["p-value"], 2)) + "$ & $" + str(round(Set["total chi^2"], 2)) + "$ & $" + str(Set["total #parameters"]) + "$ & $" + str(Set["total #dof"]) + "$ & $" + str(round(Set["bound-saturation"],2)) + '$' + '\n'
    laTex_file = open("summary.txt", 'a')
    laTex_file.write(laTex_output)
    laTex_file.close()
    print(laTex_output)

# Find mode
def cmd_find_mode(args, RETURN=False):
    analysis_file = make_analysis_file(args)
    analysis = analysis_file.analysis(args.posterior)
    min_chi2 = sys.float_info.max
    gof = None
    bfp = None
    info('Starting minimization in {} points'.format(args.points))
    starting_points = []
    if args.init_file is not '':
        info('Initializing starting point from MCMC data file')
        chain = eos.data.MarkovChain(args.init_file)
        idx_mode = _np.argmax(chain.weights)
        for p, v in zip(analysis.varied_parameters, chain.samples[idx_mode]):
            p.set(v)
    else:
        info('Using random starting point')

    for i in range(args.points):
        starting_point = [float(p) for p in analysis.varied_parameters]
        _bfp = analysis.optimize(starting_point)
        _gof = eos.GoodnessOfFit(analysis.log_posterior)
        _chi2 = _gof.total_chi_square()
        if _chi2 < min_chi2:
            gof = _gof
            bfp = _bfp
            min_chi2 = _chi2

    print('best-fit point = {}'.format(bfp.point))
    print('total chi^2 = {}'.format(min_chi2))
    print('p value = {:.1f}%'.format(100 * (1.0 - scipy.stats.chi2(gof.total_degrees_of_freedom()).cdf(gof.total_chi_square()))))
    print('individual test statistics:')
    for n, e in gof:
        print('  - {}: chi^2 / dof = {:f} / {}'.format(n, e.chi2, e.dof))
    if RETURN == True:
        return_data_dict = {"total #parameters" : len(bfp.point), "bfp" : bfp.point, "total chi^2" : min_chi2, "p-value" : 100 * (1.0 - scipy.stats.chi2(gof.total_degrees_of_freedom()).cdf(gof.total_chi_square()))}
        total_dof = 0
        for n, e in gof:
            return_data_dict[str(n)] = [e.chi2, e.dof]
            total_dof +=e.dof
        return_data_dict["total #dof"] = total_dof
        return return_data_dict

# Plot samples
def cmd_plot_samples(args):
    basename = os.path.basename(os.path.normpath(args.path))
    if basename.startswith('mcmc-'):
        data = eos.data.MarkovChain(args.path)
    elif basename.startswith('pmc'):
        data = eos.data.PMCSampler(args.path)
    else:
        raise RuntimeError('unsupported data set: {}'.format(args.path))

    parameters = eos.Parameters()
    for idx, p in enumerate(data.varied_parameters):
        print('plotting histogram for {}'.format(p['name']))
        if data.type in ['Prediction']:
            label = eos.Observables()[p['name']]
        elif data.type in ['MarkovChain', 'PMCSampler']:
            pp = parameters[p['name']]
            label = pp.latex()
        else:
            label = r'\verb+{}+'.format(p['name'])

        description = {
            'plot': {
                'x': { 'label': label, 'format':  '${x:.5f}$', 'range': [p['min'], p['max']] },
                'y': { 'label': 'prob. density' }
            },
            'contents': [
                {
                    'type': 'histogram', 'bins': args.bins,
                    'data': {
                        'samples': data.samples[:, idx],
                    }
                }
            ]
        }
        plotter = eos.plot.Plotter(description, os.path.join(args.path, '{}.pdf'.format(idx)))
        plotter.plot()

# Find clusters
def cmd_find_clusters(args):
    chains    = [eos.data.MarkovChain(path).samples for path in args.input_paths]
    means     = [_np.mean(chain, axis=0) for chain in chains]
    variances = [_np.var(chain,  axis=0) for chain in chains]
    groups    = pypmc.mix_adapt.r_value.r_group(means, variances, len(chains[0]), critical_r=args.threshold)
    info('Found {} groups using an R value threshold of {}'.format(len(groups), args.threshold))
    density   = pypmc.mix_adapt.r_value.make_r_gaussmix(chains, K_g=args.K_g, critical_r=args.threshold)
    eos.data.MixtureDensity.create(args.output_path, density)


# Sample MCMC
def cmd_sample_mcmc(args):
    output_path = os.path.join(args.base_directory, args.posterior, 'mcmc-{:04}'.format(int(args.chain)))
    set_log_file(output_path, 'log')
    analysis_file = make_analysis_file(args)
    analysis = analysis_file.analysis(args.posterior)
    rng = _np.random.mtrand.RandomState(int(args.chain) + 1701)
    try:
        samples, weights = analysis.sample(N=args.N, stride=args.stride, pre_N=args.pre_N, preruns=args.preruns, rng=rng)
        eos.data.MarkovChain.create(output_path, analysis.varied_parameters, samples, weights)
    except RuntimeError as e:
        error('encountered run time error ({e}) in parameter point:'.format(e=e))
        for p in analysis.varied_parameters:
            error(' - {n}: {v}'.format(n=p.name(), v=p.evaluate()))


# Sample PMC
def cmd_sample_pmc(args):
    analysis_file = make_analysis_file(args)
    analysis = analysis_file.analysis(args.posterior)
    rng = _np.random.mtrand.RandomState(1701)
    initial_proposal = eos.data.MixtureDensity(args.path).density()
    samples, weights, proposal = analysis.sample_pmc(initial_proposal, step_N=args.step_N, steps=args.steps, final_N=args.final_N, rng=rng)
    output_path = os.path.join(args.base_directory, args.posterior, 'pmc')
    eos.data.PMCSampler.create(output_path, analysis.varied_parameters, samples, weights, proposal)


# Predict observables
def cmd_predict_observables(args):
    _parameters = eos.Parameters()
    analysis_file = make_analysis_file(args)
    observables = analysis_file.observables(args.prediction, _parameters)

    basename = os.path.basename(os.path.normpath(args.path))
    if basename.startswith('mcmc-'):
        data = eos.data.MarkovChain(args.path)
    elif basename.startswith('pmc'):
        data = eos.data.PMCSampler(args.path)
    else:
        raise RuntimeError('unsupported data set: {}'.format(args.path))

    try:
        from tqdm import tqdm
        progressbar = tqdm
    except ImportError:
        progressbar = lambda x: x

    parameters = [_parameters[p['name']] for p in data.varied_parameters]
    observable_samples = []
    for i, sample in enumerate(progressbar(data.samples[args.begin:args.end])):
        for p, v in zip(parameters, sample):
            p.set(v)
        try:
            observable_samples.append([o.evaluate() for o in observables])
        except RuntimeError as e:
            error('skipping prediction for sample {i} due to runtime error ({e}): {s}'.format(i=i, e=e, s=sample))
            observable_samples.append([_np.nan for o in observables])
    observable_samples = _np.array(observable_samples)

    output_path = os.path.join(args.base_directory, args.prediction, 'pred')
    eos.data.Prediction.create(output_path, observables, observable_samples, data.weights[args.begin:args.end])


# List priors
def cmd_list_priors(args):
    analysis_file = make_analysis_file(args)
    for name, prior in analysis_file.priors.items():
        print(name)


# List likelihoods
def cmd_list_likelihoods(args):
    analysis_file = make_analysis_file(args)
    for name, lh in analysis_file.likelihoods.items():
        print(name)
        if not args.display:
            continue

        if not 'constraints' in lh and not 'manual_constraints' in lh:
            error('Likelihoods {name} specifies neither the \'constraints\' nor the \'manual_constraints\' key'.format(name=name))
            continue

        for c in lh['constraints'] if 'constraints' in lh else []:
            print(' - {}'.format(c))

        for mc in lh['manual_constraints'] if 'manual_constraints' in lh else {}:
            print(' - {} [manual]'.format(mc))


# List predictions
def cmd_list_predictions(args):
    analysis_file = make_analysis_file(args)
    for name, pred in analysis_file.predictions.items():
        print(name)


# List posteriors
def cmd_list_posteriors(args):
    analysis_file = make_analysis_file(args)
    for name, lh in analysis_file.posteriors.items():
        print(name)


if __name__ == '__main__':
    main()
